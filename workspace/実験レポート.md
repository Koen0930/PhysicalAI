# ロボット視覚ナビゲーション実験レポート

## 実験概要

このディレクトリでは、**Vision Language Model (VLM) を活用したロボット自律ナビゲーション**の実験を実施しました。ROS2環境下でTurtleBot3シミュレーターを使用し、カメラ画像から環境を理解してロボットを制御する統合パイプラインを開発・検証しています。

## 実験の目的

1. **視覚的ナビゲーション**: ロボットが環境内の深緑色のゴミ箱を見つけて到達する
2. **VLMの実用性検証**: QwenやGeminiなどのVLMモデルがロボット制御にどの程度有効か評価
3. **自動化パイプライン**: 画像取得→解析→コード生成→実行の完全自動化システムの構築

## パイプラインの構成と流れ

### 1. システム構成

```
カメラ画像 → VLM解析 → LLMコード生成 → ロボット制御
    ↓           ↓            ↓           ↓
  画像保存   解析結果保存  スクリプト保存  実行ログ
```

### 2. 詳細な処理フロー

#### Step 1: 画像取得・保存
- **入力**: ROS2トピック `/camera/image_raw/compressed` からカメラ画像を取得
- **処理**: 圧縮JPEG画像をOpenCVでデコード
- **出力**: タイムスタンプ付きJPG画像ファイル（例：`image_20250628_052209_001.jpg`）

#### Step 2: VLM画像解析
- **モデル**: Qwen2.5-VL-3B-Instruct または Gemini-2.5-flash
- **プロンプト**: 
  ```
  You are the visual-navigation agent for a simulated mobile robot.
  Goal: Reach the deep-green black trash can.
  Keep moving forward until the trash can occupies ≈ 90% of the field-of-view.
  ```
- **出力**: 環境の説明とロボットへの行動指示（例：「MOVE backward and TURN around」）

#### Step 3: ロボット制御コード生成
- **エンジン**: OpenAI GPT-4.1 または Gemini-2.5-flash
- **入力**: VLMの解析結果と行動指示
- **出力**: ROS2準拠のPythonスクリプト（geometry_msgs.Twist使用）

#### Step 4: コード実行
- **実行**: 生成されたPythonスクリプトを自動実行
- **制御**: `/cmd_vel`トピックにTwistメッセージをパブリッシュ
- **安全機能**: 30秒タイムアウト、停止コマンド自動送信

### 3. 主要なコンポーネント

#### `robot_vision_pipeline_hybrid.py`
- メインパイプライン制御
- ROS2ノードとしてカメラデータを受信
- VLM解析とコード生成の統合管理
- 実験データの自動保存（画像、解析結果、スクリプト）

#### `qwen_image_inference.py`
- Qwen2.5-VLモデルの推論エンジン
- 画像とテキストプロンプトから行動指示を生成
- GPU/CPU自動選択、バッチ処理対応

#### `pipeline_config.py`
- システム設定の一元管理
- VLMモデル選択、プロンプト設定
- ループ間隔、タイムアウト等のパラメータ

### 4. 行動履歴システム

パイプラインには**行動履歴機能**が実装されており：
- 過去3回の行動を記録
- VLM解析時に履歴を参考にして次の行動を決定
- 同じ行動の繰り返しを避ける仕組み

### 5. 実験制御インターフェース

```bash
# 実行時のコマンド
start  - 継続的なパイプライン実行開始
stop   - パイプライン停止
once   - 1回だけ実行
status - 現在の状態確認
quit   - 終了
```

## 技術的特徴

### ハイブリッドVLMアーキテクチャ
- **Qwen**: ローカル実行、プライバシー保護、コスト削減
- **Gemini**: クラウドAPI、高精度、最新モデル

### 完全自動化
- 人間の介入なしで連続実行
- エラー処理とリカバリー機能
- 実験データの自動保存・管理

### ROS2統合
- 標準的なROS2メッセージ形式
- 既存のロボットシステムとの互換性
- リアルタイム制御対応

## 実験の意義

この実験は、**視覚言語モデルの実用的なロボット応用**を検証する先進的な取り組みです。従来の画像認識とは異なり、自然言語での指示理解と環境認識を組み合わせることで、より柔軟で人間に近い判断能力を持つロボットシステムの実現を目指しています。

また、完全自動化されたパイプラインにより、大量の実験データを効率的に収集し、VLMの性能評価と改善に貢献する基盤を構築しています。 